{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cBBeh6142UJm",
        "outputId": "e647b959-2d70-47f3-d53a-bcaeba72acaf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pydantic in /usr/local/lib/python3.12/dist-packages (2.11.7)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic) (2.33.2)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.12/dist-packages (from pydantic) (4.14.1)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic) (0.4.1)\n"
          ]
        }
      ],
      "source": [
        "pip install pydantic"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Ak6-4N52UJs",
        "outputId": "7b18d7f1-3da7-4bcd-8596-647047bb3e2b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pydantic-ai-slim[groq]\n",
            "  Downloading pydantic_ai_slim-0.7.4-py3-none-any.whl.metadata (4.7 kB)\n",
            "Collecting eval-type-backport>=0.2.0 (from pydantic-ai-slim[groq])\n",
            "  Downloading eval_type_backport-0.2.2-py3-none-any.whl.metadata (2.2 kB)\n",
            "Collecting genai-prices>=0.0.22 (from pydantic-ai-slim[groq])\n",
            "  Downloading genai_prices-0.0.23-py3-none-any.whl.metadata (6.5 kB)\n",
            "Collecting griffe>=1.3.2 (from pydantic-ai-slim[groq])\n",
            "  Downloading griffe-1.12.1-py3-none-any.whl.metadata (5.0 kB)\n",
            "Requirement already satisfied: httpx>=0.27 in /usr/local/lib/python3.12/dist-packages (from pydantic-ai-slim[groq]) (0.28.1)\n",
            "Requirement already satisfied: opentelemetry-api>=1.28.0 in /usr/local/lib/python3.12/dist-packages (from pydantic-ai-slim[groq]) (1.36.0)\n",
            "Collecting pydantic-graph==0.7.4 (from pydantic-ai-slim[groq])\n",
            "  Downloading pydantic_graph-0.7.4-py3-none-any.whl.metadata (3.9 kB)\n",
            "Requirement already satisfied: pydantic>=2.10 in /usr/local/lib/python3.12/dist-packages (from pydantic-ai-slim[groq]) (2.11.7)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic-ai-slim[groq]) (0.4.1)\n",
            "Collecting groq>=0.25.0 (from pydantic-ai-slim[groq])\n",
            "  Downloading groq-0.31.0-py3-none-any.whl.metadata (16 kB)\n",
            "Collecting logfire-api>=3.14.1 (from pydantic-graph==0.7.4->pydantic-ai-slim[groq])\n",
            "  Downloading logfire_api-4.3.4-py3-none-any.whl.metadata (971 bytes)\n",
            "Collecting colorama>=0.4 (from griffe>=1.3.2->pydantic-ai-slim[groq])\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl.metadata (17 kB)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from groq>=0.25.0->pydantic-ai-slim[groq]) (4.10.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from groq>=0.25.0->pydantic-ai-slim[groq]) (1.9.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from groq>=0.25.0->pydantic-ai-slim[groq]) (1.3.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.10 in /usr/local/lib/python3.12/dist-packages (from groq>=0.25.0->pydantic-ai-slim[groq]) (4.14.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx>=0.27->pydantic-ai-slim[groq]) (2025.8.3)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx>=0.27->pydantic-ai-slim[groq]) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx>=0.27->pydantic-ai-slim[groq]) (3.10)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx>=0.27->pydantic-ai-slim[groq]) (0.16.0)\n",
            "Requirement already satisfied: importlib-metadata<8.8.0,>=6.0 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-api>=1.28.0->pydantic-ai-slim[groq]) (8.7.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.10->pydantic-ai-slim[groq]) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.10->pydantic-ai-slim[groq]) (2.33.2)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.12/dist-packages (from importlib-metadata<8.8.0,>=6.0->opentelemetry-api>=1.28.0->pydantic-ai-slim[groq]) (3.23.0)\n",
            "Downloading pydantic_graph-0.7.4-py3-none-any.whl (27 kB)\n",
            "Downloading eval_type_backport-0.2.2-py3-none-any.whl (5.8 kB)\n",
            "Downloading genai_prices-0.0.23-py3-none-any.whl (46 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.6/46.6 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading griffe-1.12.1-py3-none-any.whl (138 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m138.9/138.9 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading groq-0.31.0-py3-none-any.whl (131 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m131.4/131.4 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydantic_ai_slim-0.7.4-py3-none-any.whl (291 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m291.1/291.1 kB\u001b[0m \u001b[31m19.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Downloading logfire_api-4.3.4-py3-none-any.whl (88 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.4/88.4 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: logfire-api, eval-type-backport, colorama, griffe, pydantic-graph, groq, genai-prices, pydantic-ai-slim\n",
            "Successfully installed colorama-0.4.6 eval-type-backport-0.2.2 genai-prices-0.0.23 griffe-1.12.1 groq-0.31.0 logfire-api-4.3.4 pydantic-ai-slim-0.7.4 pydantic-graph-0.7.4\n"
          ]
        }
      ],
      "source": [
        "pip install \"pydantic-ai-slim[groq]\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UCpXX-pG2UJs",
        "outputId": "54afa676-ed2f-4d84-bfd3-a4aa5e2701c5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nest-asyncio in /usr/local/lib/python3.12/dist-packages (1.6.0)\n"
          ]
        }
      ],
      "source": [
        "pip install nest-asyncio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3qWsK9YM2UJu",
        "outputId": "65004d23-8cb6-4eea-8330-4781918161cc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting devtools\n",
            "  Downloading devtools-0.12.2-py3-none-any.whl.metadata (4.8 kB)\n",
            "Collecting asttokens<3.0.0,>=2.0.0 (from devtools)\n",
            "  Downloading asttokens-2.4.1-py2.py3-none-any.whl.metadata (5.2 kB)\n",
            "Collecting executing>=1.1.1 (from devtools)\n",
            "  Downloading executing-2.2.0-py2.py3-none-any.whl.metadata (8.9 kB)\n",
            "Requirement already satisfied: pygments>=2.15.0 in /usr/local/lib/python3.12/dist-packages (from devtools) (2.19.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.12/dist-packages (from asttokens<3.0.0,>=2.0.0->devtools) (1.17.0)\n",
            "Downloading devtools-0.12.2-py3-none-any.whl (19 kB)\n",
            "Downloading asttokens-2.4.1-py2.py3-none-any.whl (27 kB)\n",
            "Downloading executing-2.2.0-py2.py3-none-any.whl (26 kB)\n",
            "Installing collected packages: executing, asttokens, devtools\n",
            "Successfully installed asttokens-2.4.1 devtools-0.12.2 executing-2.2.0\n",
            "Collecting logfire\n",
            "  Downloading logfire-4.3.4-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: executing>=2.0.1 in /usr/local/lib/python3.12/dist-packages (from logfire) (2.2.0)\n",
            "Collecting opentelemetry-exporter-otlp-proto-http<1.37.0,>=1.21.0 (from logfire)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_http-1.36.0-py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting opentelemetry-instrumentation>=0.41b0 (from logfire)\n",
            "  Downloading opentelemetry_instrumentation-0.57b0-py3-none-any.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: opentelemetry-sdk<1.37.0,>=1.21.0 in /usr/local/lib/python3.12/dist-packages (from logfire) (1.36.0)\n",
            "Requirement already satisfied: protobuf>=4.23.4 in /usr/local/lib/python3.12/dist-packages (from logfire) (5.29.5)\n",
            "Requirement already satisfied: rich>=13.4.2 in /usr/local/lib/python3.12/dist-packages (from logfire) (13.9.4)\n",
            "Requirement already satisfied: typing-extensions>=4.1.0 in /usr/local/lib/python3.12/dist-packages (from logfire) (4.14.1)\n",
            "Requirement already satisfied: googleapis-common-protos~=1.52 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-exporter-otlp-proto-http<1.37.0,>=1.21.0->logfire) (1.70.0)\n",
            "Requirement already satisfied: opentelemetry-api~=1.15 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-exporter-otlp-proto-http<1.37.0,>=1.21.0->logfire) (1.36.0)\n",
            "Collecting opentelemetry-exporter-otlp-proto-common==1.36.0 (from opentelemetry-exporter-otlp-proto-http<1.37.0,>=1.21.0->logfire)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_common-1.36.0-py3-none-any.whl.metadata (1.8 kB)\n",
            "Collecting opentelemetry-proto==1.36.0 (from opentelemetry-exporter-otlp-proto-http<1.37.0,>=1.21.0->logfire)\n",
            "  Downloading opentelemetry_proto-1.36.0-py3-none-any.whl.metadata (2.3 kB)\n",
            "Requirement already satisfied: requests~=2.7 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-exporter-otlp-proto-http<1.37.0,>=1.21.0->logfire) (2.32.4)\n",
            "Requirement already satisfied: opentelemetry-semantic-conventions==0.57b0 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-instrumentation>=0.41b0->logfire) (0.57b0)\n",
            "Requirement already satisfied: packaging>=18.0 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-instrumentation>=0.41b0->logfire) (25.0)\n",
            "Requirement already satisfied: wrapt<2.0.0,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-instrumentation>=0.41b0->logfire) (1.17.3)\n",
            "Requirement already satisfied: importlib-metadata<8.8.0,>=6.0 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-api~=1.15->opentelemetry-exporter-otlp-proto-http<1.37.0,>=1.21.0->logfire) (8.7.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich>=13.4.2->logfire) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich>=13.4.2->logfire) (2.19.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich>=13.4.2->logfire) (0.1.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests~=2.7->opentelemetry-exporter-otlp-proto-http<1.37.0,>=1.21.0->logfire) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests~=2.7->opentelemetry-exporter-otlp-proto-http<1.37.0,>=1.21.0->logfire) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests~=2.7->opentelemetry-exporter-otlp-proto-http<1.37.0,>=1.21.0->logfire) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests~=2.7->opentelemetry-exporter-otlp-proto-http<1.37.0,>=1.21.0->logfire) (2025.8.3)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.12/dist-packages (from importlib-metadata<8.8.0,>=6.0->opentelemetry-api~=1.15->opentelemetry-exporter-otlp-proto-http<1.37.0,>=1.21.0->logfire) (3.23.0)\n",
            "Downloading logfire-4.3.4-py3-none-any.whl (213 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m213.8/213.8 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_exporter_otlp_proto_http-1.36.0-py3-none-any.whl (18 kB)\n",
            "Downloading opentelemetry_exporter_otlp_proto_common-1.36.0-py3-none-any.whl (18 kB)\n",
            "Downloading opentelemetry_proto-1.36.0-py3-none-any.whl (72 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.5/72.5 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_instrumentation-0.57b0-py3-none-any.whl (32 kB)\n",
            "Installing collected packages: opentelemetry-proto, opentelemetry-exporter-otlp-proto-common, opentelemetry-instrumentation, opentelemetry-exporter-otlp-proto-http, logfire\n",
            "Successfully installed logfire-4.3.4 opentelemetry-exporter-otlp-proto-common-1.36.0 opentelemetry-exporter-otlp-proto-http-1.36.0 opentelemetry-instrumentation-0.57b0 opentelemetry-proto-1.36.0\n"
          ]
        }
      ],
      "source": [
        "!pip install devtools\n",
        "!pip install logfire"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-AYF9-WN2UJq",
        "outputId": "c9fe50f1-1823-4dea-f7ef-ee98aa4170d5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "product_name='กาแฟสด' max_price=1000.0\n"
          ]
        }
      ],
      "source": [
        "from pydantic import BaseModel, PositiveFloat\n",
        "\n",
        "class ProductQuery(BaseModel):\n",
        "    product_name: str\n",
        "    max_price: PositiveFloat\n",
        "\n",
        "# ข้อมูลถูกต้องตามเงื่อนไขที่กำหนด\n",
        "query = ProductQuery(product_name=\"กาแฟสด\", max_price=1000.0)\n",
        "print(query)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 325
        },
        "id": "mWQfq8Xl2UJr",
        "outputId": "3943bf51-075b-4e8b-d784-04cc7bdc3b01"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValidationError",
          "evalue": "1 validation error for ProductQuery\nmax_price\n  Input should be greater than 0 [type=greater_than, input_value=0.0, input_type=float]\n    For further information visit https://errors.pydantic.dev/2.11/v/greater_than",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValidationError\u001b[0m                           Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3096925529.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mquery\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mProductQuery\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproduct_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"กาแฟสด\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_price\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pydantic/main.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, **data)\u001b[0m\n\u001b[1;32m    251\u001b[0m         \u001b[0;31m# `__tracebackhide__` tells pytest and some other tools to omit this function from tracebacks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m         \u001b[0m__tracebackhide__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 253\u001b[0;31m         \u001b[0mvalidated_self\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__pydantic_validator__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalidate_python\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself_instance\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    254\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mvalidated_self\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m             warnings.warn(\n",
            "\u001b[0;31mValidationError\u001b[0m: 1 validation error for ProductQuery\nmax_price\n  Input should be greater than 0 [type=greater_than, input_value=0.0, input_type=float]\n    For further information visit https://errors.pydantic.dev/2.11/v/greater_than"
          ]
        }
      ],
      "source": [
        "query = ProductQuery(product_name=\"กาแฟสด\", max_price=0.0)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 325
        },
        "id": "5vhZ0bD22UJr",
        "outputId": "fe45e2ed-d52c-4632-e84d-7d2b8db0fe54"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValidationError",
          "evalue": "1 validation error for ProductQuery\nmax_price\n  Input should be greater than 0 [type=greater_than, input_value=-500.0, input_type=float]\n    For further information visit https://errors.pydantic.dev/2.11/v/greater_than",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValidationError\u001b[0m                           Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3188409406.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mquery\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mProductQuery\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproduct_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"กาแฟสด\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_price\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m500.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pydantic/main.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, **data)\u001b[0m\n\u001b[1;32m    251\u001b[0m         \u001b[0;31m# `__tracebackhide__` tells pytest and some other tools to omit this function from tracebacks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m         \u001b[0m__tracebackhide__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 253\u001b[0;31m         \u001b[0mvalidated_self\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__pydantic_validator__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalidate_python\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself_instance\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    254\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mvalidated_self\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m             warnings.warn(\n",
            "\u001b[0;31mValidationError\u001b[0m: 1 validation error for ProductQuery\nmax_price\n  Input should be greater than 0 [type=greater_than, input_value=-500.0, input_type=float]\n    For further information visit https://errors.pydantic.dev/2.11/v/greater_than"
          ]
        }
      ],
      "source": [
        "query = ProductQuery(product_name=\"กาแฟสด\", max_price=-500.0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "ZbOoJrhx2UJs"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "groq_key = userdata.get('GROQ_CHULA')\n",
        "\n",
        "os.environ[\"GROQ_API_KEY\"] = groq_key\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "u13T9Pzw2UJt"
      },
      "outputs": [],
      "source": [
        "import nest_asyncio\n",
        "nest_asyncio.apply()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9ORrzfze2UJt",
        "outputId": "9a1946cd-87ad-450f-fba0-475682004437"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The capital of England is London.\n"
          ]
        }
      ],
      "source": [
        "from pydantic_ai import Agent\n",
        "\n",
        "\n",
        "agent = Agent(\n",
        "    'groq:openai/gpt-oss-20b',\n",
        "    system_prompt='Be concise, reply with one sentence.',\n",
        ")\n",
        "\n",
        "\n",
        "result = agent.run_sync('Where is the capital of England')\n",
        "print(result.output)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pydantic_ai import Agent\n",
        "\n",
        "\n",
        "agent = Agent(\n",
        "    'groq:openai/gpt-oss-20b',\n",
        "    system_prompt='Be concise, reply in Thai with one sentence.',\n",
        ")\n",
        "\n",
        "\n",
        "result = agent.run_sync('Where is the capital of England')\n",
        "print(result.output)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6uDrcuXzdaYW",
        "outputId": "0eaaaef1-b5a9-433b-a740-4fb8f1792269"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ลอนดอนเป็นเมืองหลวงของอังกฤษและตั้งอยู่ที่ส่วนตะวันออกเฉียงใต้ของประเทศบนแม่น้ำเทมส์.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "mJ3NoRAf2UJu"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "os.environ[\"WEATHER_API_KEY\"] = userdata.get('WEATHER_API_KEY').strip()\n",
        "os.environ[\"GEO_API_KEY\"] = userdata.get('GEO_API_KEY').strip()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mxj3i73M2UJu",
        "outputId": "c50114b2-8dcb-4886-c66f-ce15a1bd07da"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "06:40:19.683 calling geocode API\n",
            "06:40:20.388 calling weather API\n",
            "06:40:21.366 calling geocode API\n",
            "06:40:21.998 calling weather API\n",
            "/tmp/ipython-input-650333967.py:131 main\n",
            "    result: AgentRunResult(\n",
            "        output='กรุงเทพฯ 33\\u202f°C หมอกเบา ในนิวเจอร์ซีย์ 14\\u202f°C แดดจ้า',\n",
            "        _output_tool_name=None,\n",
            "        _state=GraphAgentState(\n",
            "            message_history=[\n",
            "                ModelRequest(\n",
            "                    parts=[\n",
            "                        SystemPromptPart(\n",
            "                            content='Be concise, reply in Thai with one sentence.',\n",
            "                            timestamp=datetime.datetime(2025, 8, 22, 6, 40, 19, 181900, tzinfo=datetime.timezone.utc),\n",
            "                            dynamic_ref=None,\n",
            "                            part_kind='system-prompt',\n",
            "                        ),\n",
            "                        UserPromptPart(\n",
            "                            content='What is the weather like in Bangkok and in New Jersey?',\n",
            "                            timestamp=datetime.datetime(2025, 8, 22, 6, 40, 19, 181910, tzinfo=datetime.timezone.utc),\n",
            "                            part_kind='user-prompt',\n",
            "                        ),\n",
            "                    ],\n",
            "                    instructions=None,\n",
            "                    kind='request',\n",
            "                ),\n",
            "                ModelResponse(\n",
            "                    parts=[\n",
            "                        ThinkingPart(\n",
            "                            content=(\n",
            "                                'We need to provide weather for Bangkok and New Jersey. Use functions. First get coord'\n",
            "                                'inates for Bangkok, then for New Jersey. Use get_lat_lng for each location, then get_'\n",
            "                                \"weather for each coordinate. We'll need two separate calls? We can do sequential. Fir\"\n",
            "                                'st call get_lat_lng with location \"Bangkok\". Then get_weather with returned lat/lng. '\n",
            "                                'Then get_lat_lng for \"New Jersey\". Then get_weather. Then compose answer. Need to com'\n",
            "                                'ply with developer instruction: be concise, reply in Thai with one sentence. So final'\n",
            "                                ' answer: single Thai sentence summarizing weather. But we need data: we can call func'\n",
            "                                \"tions to get data. We'll produce result.\\n\"\n",
            "                                '\\n'\n",
            "                                'Let\\'s first call get_lat_lng for \"Bangkok\".'\n",
            "                            ),\n",
            "                            id=None,\n",
            "                            signature=None,\n",
            "                            part_kind='thinking',\n",
            "                        ),\n",
            "                        ToolCallPart(\n",
            "                            tool_name='get_lat_lng',\n",
            "                            args='{\"location_description\":\"Bangkok\"}',\n",
            "                            tool_call_id='fc_94822704-e0ba-4d2d-9870-ef1565baa6a8',\n",
            "                            part_kind='tool-call',\n",
            "                        ),\n",
            "                    ],\n",
            "                    usage=RequestUsage(\n",
            "                        input_tokens=192,\n",
            "                        cache_write_tokens=0,\n",
            "                        cache_read_tokens=0,\n",
            "                        output_tokens=176,\n",
            "                        input_audio_tokens=0,\n",
            "                        cache_audio_read_tokens=0,\n",
            "                        output_audio_tokens=0,\n",
            "                        details={},\n",
            "                    ),\n",
            "                    model_name='openai/gpt-oss-20b',\n",
            "                    timestamp=datetime.datetime(2025, 8, 22, 6, 40, 19, tzinfo=TzInfo(UTC)),\n",
            "                    kind='response',\n",
            "                    provider_details=None,\n",
            "                    provider_request_id='chatcmpl-1d14c174-2673-4737-8ae3-33c02d59f063',\n",
            "                ),\n",
            "                ModelRequest(\n",
            "                    parts=[\n",
            "                        ToolReturnPart(\n",
            "                            tool_name='get_lat_lng',\n",
            "                            content={\n",
            "                                'lat': '13.7524938',\n",
            "                                'lng': '100.4935089',\n",
            "                            },\n",
            "                            tool_call_id='fc_94822704-e0ba-4d2d-9870-ef1565baa6a8',\n",
            "                            metadata=None,\n",
            "                            timestamp=datetime.datetime(2025, 8, 22, 6, 40, 20, 56403, tzinfo=datetime.timezone.utc),\n",
            "                            part_kind='tool-return',\n",
            "                        ),\n",
            "                    ],\n",
            "                    instructions=None,\n",
            "                    kind='request',\n",
            "                ),\n",
            "                ModelResponse(\n",
            "                    parts=[\n",
            "                        ThinkingPart(\n",
            "                            content=\"We need weather for Bangkok and New Jersey. We'll call get_weather for each.\",\n",
            "                            id=None,\n",
            "                            signature=None,\n",
            "                            part_kind='thinking',\n",
            "                        ),\n",
            "                        ToolCallPart(\n",
            "                            tool_name='get_weather',\n",
            "                            args='{\"lat\":13.7524938,\"lng\":100.4935089}',\n",
            "                            tool_call_id='fc_a7a1d2c0-7cc7-4cd1-bad8-3ab2a2e7d75f',\n",
            "                            part_kind='tool-call',\n",
            "                        ),\n",
            "                    ],\n",
            "                    usage=RequestUsage(\n",
            "                        input_tokens=239,\n",
            "                        cache_write_tokens=0,\n",
            "                        cache_read_tokens=0,\n",
            "                        output_tokens=52,\n",
            "                        input_audio_tokens=0,\n",
            "                        cache_audio_read_tokens=0,\n",
            "                        output_audio_tokens=0,\n",
            "                        details={},\n",
            "                    ),\n",
            "                    model_name='openai/gpt-oss-20b',\n",
            "                    timestamp=datetime.datetime(2025, 8, 22, 6, 40, 20, tzinfo=TzInfo(UTC)),\n",
            "                    kind='response',\n",
            "                    provider_details=None,\n",
            "                    provider_request_id='chatcmpl-088a13cd-fb5e-4d1e-b7d9-54cfc76ff886',\n",
            "                ),\n",
            "                ModelRequest(\n",
            "                    parts=[\n",
            "                        ToolReturnPart(\n",
            "                            tool_name='get_weather',\n",
            "                            content={\n",
            "                                'temperature': '33°C',\n",
            "                                'description': 'Light Fog',\n",
            "                            },\n",
            "                            tool_call_id='fc_a7a1d2c0-7cc7-4cd1-bad8-3ab2a2e7d75f',\n",
            "                            metadata=None,\n",
            "                            timestamp=datetime.datetime(2025, 8, 22, 6, 40, 20, 593203, tzinfo=datetime.timezone.utc),\n",
            "                            part_kind='tool-return',\n",
            "                        ),\n",
            "                    ],\n",
            "                    instructions=None,\n",
            "                    kind='request',\n",
            "                ),\n",
            "                ModelResponse(\n",
            "                    parts=[\n",
            "                        ThinkingPart(\n",
            "                            content=(\n",
            "                                'We have responded with the weather for Bangkok but not for New Jersey. We need to do '\n",
            "                                'same for New Jersey. The user asked: \"What is the weather like in Bangkok and in New '\n",
            "                                'Jersey?\" We have to reply in Thai with one sentence. We need to provide weather for b'\n",
            "                                'oth. We need to get lat/lng for New Jersey. Let\\'s do function call get_lat_lng for \"N'\n",
            "                                'ew Jersey\".'\n",
            "                            ),\n",
            "                            id=None,\n",
            "                            signature=None,\n",
            "                            part_kind='thinking',\n",
            "                        ),\n",
            "                    ],\n",
            "                    usage=RequestUsage(\n",
            "                        input_tokens=286,\n",
            "                        cache_write_tokens=0,\n",
            "                        cache_read_tokens=0,\n",
            "                        output_tokens=102,\n",
            "                        input_audio_tokens=0,\n",
            "                        cache_audio_read_tokens=0,\n",
            "                        output_audio_tokens=0,\n",
            "                        details={},\n",
            "                    ),\n",
            "                    model_name='openai/gpt-oss-20b',\n",
            "                    timestamp=datetime.datetime(2025, 8, 22, 6, 40, 20, tzinfo=TzInfo(UTC)),\n",
            "                    kind='response',\n",
            "                    provider_details=None,\n",
            "                    provider_request_id='chatcmpl-44fa172e-b7e4-4165-afba-e208b22e3ca5',\n",
            "                ),\n",
            "                ModelRequest(\n",
            "                    parts=[\n",
            "                        RetryPromptPart(\n",
            "                            content='Responses without text or tool calls are not permitted.',\n",
            "                            tool_name=None,\n",
            "                            tool_call_id='pyd_ai_7025e1be30a54ea59a2bd4aa15484bf5',\n",
            "                            timestamp=datetime.datetime(2025, 8, 22, 6, 40, 20, 974550, tzinfo=datetime.timezone.utc),\n",
            "                            part_kind='retry-prompt',\n",
            "                        ),\n",
            "                    ],\n",
            "                    instructions=None,\n",
            "                    kind='request',\n",
            "                ),\n",
            "                ModelResponse(\n",
            "                    parts=[\n",
            "                        ThinkingPart(\n",
            "                            content=(\n",
            "                                'We need to respond with a single sentence in Thai. The request: \"What is the weather '\n",
            "                                'like in Bangkok and in New Jersey?\" We need to give answer in Thai in one sentence. A'\n",
            "                                'lso must include tool calls for each location. We need to fetch weather for each. Alr'\n",
            "                                'eady did for Bangkok. Need also for New Jersey. Use get_lat_lng for New Jersey (maybe'\n",
            "                                \" Newark). Then get_weather. Then produce sentence. We'll do two tool calls.\"\n",
            "                            ),\n",
            "                            id=None,\n",
            "                            signature=None,\n",
            "                            part_kind='thinking',\n",
            "                        ),\n",
            "                        ToolCallPart(\n",
            "                            tool_name='get_lat_lng',\n",
            "                            args='{\"location_description\":\"New Jersey\"}',\n",
            "                            tool_call_id='fc_6c2f7dd7-a17e-4f87-ac15-61e96af0f7e0',\n",
            "                            part_kind='tool-call',\n",
            "                        ),\n",
            "                    ],\n",
            "                    usage=RequestUsage(\n",
            "                        input_tokens=316,\n",
            "                        cache_write_tokens=0,\n",
            "                        cache_read_tokens=0,\n",
            "                        output_tokens=120,\n",
            "                        input_audio_tokens=0,\n",
            "                        cache_audio_read_tokens=0,\n",
            "                        output_audio_tokens=0,\n",
            "                        details={},\n",
            "                    ),\n",
            "                    model_name='openai/gpt-oss-20b',\n",
            "                    timestamp=datetime.datetime(2025, 8, 22, 6, 40, 21, tzinfo=TzInfo(UTC)),\n",
            "                    kind='response',\n",
            "                    provider_details=None,\n",
            "                    provider_request_id='chatcmpl-547b1ead-1e3c-453d-b571-c9d20303964b',\n",
            "                ),\n",
            "                ModelRequest(\n",
            "                    parts=[\n",
            "                        ToolReturnPart(\n",
            "                            tool_name='get_lat_lng',\n",
            "                            content={\n",
            "                                'lat': '40.0757384',\n",
            "                                'lng': '-74.4041622',\n",
            "                            },\n",
            "                            tool_call_id='fc_6c2f7dd7-a17e-4f87-ac15-61e96af0f7e0',\n",
            "                            metadata=None,\n",
            "                            timestamp=datetime.datetime(2025, 8, 22, 6, 40, 21, 593101, tzinfo=datetime.timezone.utc),\n",
            "                            part_kind='tool-return',\n",
            "                        ),\n",
            "                    ],\n",
            "                    instructions=None,\n",
            "                    kind='request',\n",
            "                ),\n",
            "                ModelResponse(\n",
            "                    parts=[\n",
            "                        ThinkingPart(\n",
            "                            content=(\n",
            "                                'We need to produce a response in Thai, one sentence, concise. We must include the wea'\n",
            "                                'ther for Bangkok and New Jersey. We have weather for Bangkok from previous call: 33°C'\n",
            "                                \", Light Fog. We haven't called get_weather for New Jersey. We need to get weather for\"\n",
            "                                ' New Jersey: call get_weather with lat 40.0757384, lng -74.4041622. Then produce a se'\n",
            "                                'ntence in Thai summarizing both. So next call.'\n",
            "                            ),\n",
            "                            id=None,\n",
            "                            signature=None,\n",
            "                            part_kind='thinking',\n",
            "                        ),\n",
            "                        ToolCallPart(\n",
            "                            tool_name='get_weather',\n",
            "                            args='{\"lat\":40.0757384,\"lng\":-74.4041622}',\n",
            "                            tool_call_id='fc_7e7446e4-1e5a-44b2-9bb0-e2c628e82094',\n",
            "                            part_kind='tool-call',\n",
            "                        ),\n",
            "                    ],\n",
            "                    usage=RequestUsage(\n",
            "                        input_tokens=364,\n",
            "                        cache_write_tokens=0,\n",
            "                        cache_read_tokens=0,\n",
            "                        output_tokens=127,\n",
            "                        input_audio_tokens=0,\n",
            "                        cache_audio_read_tokens=0,\n",
            "                        output_audio_tokens=0,\n",
            "                        details={},\n",
            "                    ),\n",
            "                    model_name='openai/gpt-oss-20b',\n",
            "                    timestamp=datetime.datetime(2025, 8, 22, 6, 40, 21, tzinfo=TzInfo(UTC)),\n",
            "                    kind='response',\n",
            "                    provider_details=None,\n",
            "                    provider_request_id='chatcmpl-b2a74db8-d414-4465-a77a-c5c2473e3b73',\n",
            "                ),\n",
            "                ModelRequest(\n",
            "                    parts=[\n",
            "                        ToolReturnPart(\n",
            "                            tool_name='get_weather',\n",
            "                            content={\n",
            "                                'temperature': '14°C',\n",
            "                                'description': 'Clear, Sunny',\n",
            "                            },\n",
            "                            tool_call_id='fc_7e7446e4-1e5a-44b2-9bb0-e2c628e82094',\n",
            "                            metadata=None,\n",
            "                            timestamp=datetime.datetime(2025, 8, 22, 6, 40, 22, 88015, tzinfo=datetime.timezone.utc),\n",
            "                            part_kind='tool-return',\n",
            "                        ),\n",
            "                    ],\n",
            "                    instructions=None,\n",
            "                    kind='request',\n",
            "                ),\n",
            "                ModelResponse(\n",
            "                    parts=[\n",
            "                        ThinkingPart(\n",
            "                            content=(\n",
            "                                'The user says \"Validation feedback: Responses without text or tool calls are not perm'\n",
            "                                'itted. Fix the errors and try again.\" They had no text in their replies; they only ma'\n",
            "                                'de tool calls. The instructions: \"Be concise, reply in Thai with one sentence.\" So I '\n",
            "                                'need to provide a single Thai sentence summarizing the weather in Bangkok and New Jer'\n",
            "                                'sey. Must include the information: temperature and description. Use concise Thai. For'\n",
            "                                ' example: \"อากาศในกรุงเทพฯ มีอุณหภูมิ 33\\u202f°C พร้อมหมอกเบา และในรัฐนิวยอร์กอาจารย์อุณหภ'\n",
            "                                'ูมิ 14\\u202f°C มีความฝันแจ่มใส\" Wait, we need to refer to New Jersey. In Thai: \"อากาศในกรุ'\n",
            "                                'งเทพฯ 33\\u202f°C หมอกเบา ส่วนในนิวเจอร์ซีย์ 14\\u202f°C แดดจ้า\" That is one sentence? It\\'s two c'\n",
            "                                'lauses but still one sentence. We can keep it short: \"กรุงเทพฯ 33\\u202f°C หมอกเบา ในนิวเจอ'\n",
            "                                'ร์ซีย์ 14\\u202f°C แดดจ้า\". That is one sentence. Good.'\n",
            "                            ),\n",
            "                            id=None,\n",
            "                            signature=None,\n",
            "                            part_kind='thinking',\n",
            "                        ),\n",
            "                        TextPart(\n",
            "                            content='กรุงเทพฯ 33\\u202f°C หมอกเบา ในนิวเจอร์ซีย์ 14\\u202f°C แดดจ้า',\n",
            "                            part_kind='text',\n",
            "                        ),\n",
            "                    ],\n",
            "                    usage=RequestUsage(\n",
            "                        input_tokens=413,\n",
            "                        cache_write_tokens=0,\n",
            "                        cache_read_tokens=0,\n",
            "                        output_tokens=285,\n",
            "                        input_audio_tokens=0,\n",
            "                        cache_audio_read_tokens=0,\n",
            "                        output_audio_tokens=0,\n",
            "                        details={},\n",
            "                    ),\n",
            "                    model_name='openai/gpt-oss-20b',\n",
            "                    timestamp=datetime.datetime(2025, 8, 22, 6, 40, 22, tzinfo=TzInfo(UTC)),\n",
            "                    kind='response',\n",
            "                    provider_details=None,\n",
            "                    provider_request_id='chatcmpl-c5f9240d-5967-4777-943f-8f170772f2d8',\n",
            "                ),\n",
            "            ],\n",
            "            usage=RunUsage(\n",
            "                input_tokens=1810,\n",
            "                cache_write_tokens=0,\n",
            "                cache_read_tokens=0,\n",
            "                output_tokens=862,\n",
            "                input_audio_tokens=0,\n",
            "                cache_audio_read_tokens=0,\n",
            "                output_audio_tokens=0,\n",
            "                details={},\n",
            "                requests=6,\n",
            "            ),\n",
            "            retries=0,\n",
            "            run_step=6,\n",
            "        ),\n",
            "        _new_message_index=0,\n",
            "        _traceparent_value=None,\n",
            "    ) (AgentRunResult)\n",
            "Response: กรุงเทพฯ 33 °C หมอกเบา ในนิวเจอร์ซีย์ 14 °C แดดจ้า\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "from __future__ import annotations as _annotations\n",
        "\n",
        "import asyncio\n",
        "import os\n",
        "from dataclasses import dataclass\n",
        "from typing import Any\n",
        "\n",
        "import logfire\n",
        "from devtools import debug\n",
        "from httpx import AsyncClient\n",
        "\n",
        "from pydantic_ai import Agent, ModelRetry, RunContext\n",
        "\n",
        "\n",
        "logfire.configure(send_to_logfire='if-token-present')\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class Deps:\n",
        "    client: AsyncClient\n",
        "    weather_api_key: str | None\n",
        "    geo_api_key: str | None\n",
        "\n",
        "\n",
        "weather_agent = Agent(\n",
        "    'groq:openai/gpt-oss-20b',\n",
        "    system_prompt='Be concise, reply in Thai with one sentence.',\n",
        ")\n",
        "\n",
        "\n",
        "@weather_agent.tool\n",
        "async def get_lat_lng(\n",
        "    ctx: RunContext[Deps], location_description: str\n",
        ") -> dict[str, float]:\n",
        "    \"\"\"Get the latitude and longitude of a location.\n",
        "\n",
        "    Args:\n",
        "        ctx: The context.\n",
        "        location_description: A description of a location.\n",
        "    \"\"\"\n",
        "    if ctx.deps.geo_api_key is None:\n",
        "        return {'lat': 51.1, 'lng': -0.1}\n",
        "\n",
        "    params = {\n",
        "        'q': location_description,\n",
        "        'api_key': ctx.deps.geo_api_key,\n",
        "    }\n",
        "    with logfire.span('calling geocode API', params=params) as span:\n",
        "        r = await ctx.deps.client.get('https://geocode.maps.co/search', params=params)\n",
        "        r.raise_for_status()\n",
        "        data = r.json()\n",
        "        span.set_attribute('response', data)\n",
        "\n",
        "    if data:\n",
        "        return {'lat': data[0]['lat'], 'lng': data[0]['lon']}\n",
        "    else:\n",
        "        raise ModelRetry('Could not find the location')\n",
        "\n",
        "\n",
        "@weather_agent.tool\n",
        "async def get_weather(ctx: RunContext[Deps], lat: float, lng: float) -> dict[str, Any]:\n",
        "    \"\"\"Get the weather at a location.\n",
        "\n",
        "    Args:\n",
        "        ctx: The context.\n",
        "        lat: Latitude of the location.\n",
        "        lng: Longitude of the location.\n",
        "    \"\"\"\n",
        "    if ctx.deps.weather_api_key is None:\n",
        "        # if no API key is provided, return a dummy response\n",
        "        return {'temperature': '21 °C', 'description': 'Sunny'}\n",
        "\n",
        "    params = {\n",
        "        'apikey': ctx.deps.weather_api_key,\n",
        "        'location': f'{lat},{lng}',\n",
        "        'units': 'metric',\n",
        "    }\n",
        "    with logfire.span('calling weather API', params=params) as span:\n",
        "        r = await ctx.deps.client.get(\n",
        "            'https://api.tomorrow.io/v4/weather/realtime', params=params\n",
        "        )\n",
        "        r.raise_for_status()\n",
        "        data = r.json()\n",
        "        span.set_attribute('response', data)\n",
        "\n",
        "    values = data['data']['values']\n",
        "    # https://docs.tomorrow.io/reference/data-layers-weather-codes\n",
        "    code_lookup = {\n",
        "        1000: 'Clear, Sunny',\n",
        "        1100: 'Mostly Clear',\n",
        "        1101: 'Partly Cloudy',\n",
        "        1102: 'Mostly Cloudy',\n",
        "        1001: 'Cloudy',\n",
        "        2000: 'Fog',\n",
        "        2100: 'Light Fog',\n",
        "        4000: 'Drizzle',\n",
        "        4001: 'Rain',\n",
        "        4200: 'Light Rain',\n",
        "        4201: 'Heavy Rain',\n",
        "        5000: 'Snow',\n",
        "        5001: 'Flurries',\n",
        "        5100: 'Light Snow',\n",
        "        5101: 'Heavy Snow',\n",
        "        6000: 'Freezing Drizzle',\n",
        "        6001: 'Freezing Rain',\n",
        "        6200: 'Light Freezing Rain',\n",
        "        6201: 'Heavy Freezing Rain',\n",
        "        7000: 'Ice Pellets',\n",
        "        7101: 'Heavy Ice Pellets',\n",
        "        7102: 'Light Ice Pellets',\n",
        "        8000: 'Thunderstorm',\n",
        "    }\n",
        "    return {\n",
        "        'temperature': f'{values[\"temperatureApparent\"]:0.0f}°C',\n",
        "        'description': code_lookup.get(values['weatherCode'], 'Unknown'),\n",
        "    }\n",
        "\n",
        "\n",
        "async def main():\n",
        "    async with AsyncClient() as client:\n",
        "        # create a free API key at https://www.tomorrow.io/weather-api/\n",
        "        weather_api_key = os.getenv('WEATHER_API_KEY')\n",
        "        # create a free API key at https://geocode.maps.co/\n",
        "        geo_api_key = os.getenv('GEO_API_KEY')\n",
        "        deps = Deps(\n",
        "            client=client, weather_api_key=weather_api_key, geo_api_key=geo_api_key\n",
        "        )\n",
        "        result = await weather_agent.run(\n",
        "            'What is the weather like in Bangkok and in New Jersey?', deps=deps\n",
        "        )\n",
        "        debug(result)\n",
        "        print('Response:', result.output)\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    asyncio.run(main())"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}